{
  "paragraphs": [
    {
      "text": "%md ## 评价类数据点击ugc占比,非ugc点击站点分布，医疗和非医疗占比",
      "dateUpdated": "Jul 17, 2018 4:15:08 PM",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003e采样评价和选择问答案分类准确率\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308869_1857932236",
      "id": "20180426-125946_729598550",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " def removeCbrackets(line : String) : String \u003d {\r\n    try {\r\n      return  line replaceAll (\"\u003c.*?\u003e\", \"\")\r\n    }\r\n    catch {\r\n      case ex: Exception \u003d\u003e \"\"\r\n    }\r\n  }\r\n\r\n  \r\n  def removePunc(line : String) : String \u003d {\r\n    try {\r\n      return  line replaceAll (\"[\\\\s+\\\\.\\\\!\\\\/_,，。？\\\\$%\\\\^\\\\*\\\\\\\"\\\u0027+｀｀`\\\\｀·——《》｀()?【】:：;～；;“”！，。？、~@#￥%……\u0026*（）]+\", \"\")\r\n    }\r\n    catch {\r\n      case ex: Exception \u003d\u003e \"\"\r\n    }\r\n  }\r\n\r\n  def hash(s:String)\u003d{\r\n    val m \u003d java.security.MessageDigest.getInstance(\"MD5\")\r\n    val b \u003d s.getBytes(\"UTF-8\")\r\n    m.update(b,0,b.length)\r\n    new java.math.BigInteger(1,m.digest()).toString(16).trim()\r\n  }\r\n  \r\n  val queryHash \u003d (s: String) \u003d\u003e{\r\n    var ask \u003d s.trim()\r\n    ask \u003d removeCbrackets(ask)\r\n    ask \u003d removePunc(ask)\r\n    var ID \u003d hash(ask)\r\n    ID\r\n  }\r\n  \r\n \r\n  def lineToQA(line : String)\u003d {\r\n\r\n    val regex \u003d \"@[A-Za-z._]{1,30}:\".r\r\n    val tags \u003d regex findAllIn line toArray\r\n    val contents \u003d line.split(\"@[A-Za-z._]{1,30}:\",-1)\r\n   \r\n  \r\n    var ans \u003d \"\"\r\n    var ID \u003d \"\"\r\n    var ask\u003d\"\"\r\n    try{\r\n      for(i\u003c-0 until tags.length){\r\n        var tag \u003d tags(i)\r\n        var content \u003d contents(i+1)\r\n     \r\n        if(tag\u003d\u003d\"@ANS.CONTENT:\"){\r\n          ans \u003d content.trim()\r\n        }else if(tag\u003d\u003d\"@ASK.TITLE:\"){\r\n          ask \u003d content.trim()\r\n          ask \u003d removeCbrackets(ask)\r\n          ask \u003d removePunc(ask)\r\n          ID \u003d hash(ask)\r\n        }\r\n\r\n        //     M +\u003d (tag -\u003e content)\r\n      }\r\n    }catch{\r\n      case e: Exception \u003d\u003e {\r\n        ask \u003d \"\"\r\n        ans \u003d \"\"\r\n        ID \u003d \"\"\r\n      }\r\n    }\r\n    (ask,ID,ans)\r\n  }\r\n  \r\n  \r\n  def lineToM(l : String)\u003d {\r\n    var line \u003d l\r\n    if(line.startsWith(\"\\\"\")\u0026\u0026line.endsWith(\"\\\"\")){\r\n      line \u003d line.substring(1,line.length-1)\r\n    }\r\n    val regex \u003d \"@[A-Za-z._]{1,30}:\".r\r\n    val tags \u003d regex findAllIn line toArray\r\n    val contents \u003d line.split(\"@[A-Za-z._]{1,30}:\",-1)\r\n    \r\n    var M:Map[String,String] \u003d Map()\r\n    \r\n    for(i\u003c-0 until tags.length){\r\n      var tag \u003d tags(i)\r\n      var content \u003d contents(i+1)\r\n      M +\u003d (tag -\u003e content.trim())\r\n    }\r\n    M\r\n  }\r\n  \r\n  \r\n   ",
      "user": "anonymous",
      "dateUpdated": "Jul 18, 2018 1:03:20 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": "org.apache.zeppelin.interpreter.InterpreterException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: 拒绝连接 (Connection refused)",
      "apps": [],
      "jobName": "paragraph_1531815308871_1858701734",
      "id": "20180425-182839_1949268193",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 18, 2018 1:03:20 PM",
      "dateFinished": "Jul 18, 2018 1:03:20 PM",
      "status": "ERROR",
      "errorMessage": "java.net.ConnectException: 拒绝连接 (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:589)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:182)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:92)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:352)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:406)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:329)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.net.{URLDecoder, URLEncoder}\r\nimport java.text.SimpleDateFormat\r\nimport java.util.Date\r\nimport scala.util.matching.Regex\r\n\r\ncase class Logrow(userid:String,uuid:String, page:String, time : String, search_type : String, query : String,urls:List[Map[String,String]])\r\ncase class DFrow(query: String,clk_ugc:Int,clk_o:Int)\r\n\r\ndef qaviewFilter(line:String):Boolean\u003d{\r\n    try {\r\n    val patterns: List[String] \u003d List(\"(?:是否|能否|可否|应否|该不该|会不会|可不可以|能不能|是不是|要不要|应不应|应不应该|有么有|有没|有没有|有木有|有无|还是不|还是没)\", \"(?:可以吗|可以么|行不行|好不好|如何|怎样|好吗|怎么样|肿么样|咋样|好么|行吗|好嘛|好不|何如|刻印吗|厉害吗|行么|真的吗)$\", \"(?:是|要|能|不能|可以|能够|还能|有|会|可能|能用|含有|也会).+?(?:吗|吧|么|嘛|不)$\")\r\n    for (pattern \u003c- patterns) {\r\n      var patternp \u003d new Regex(pattern)\r\n      if(patternp.findFirstMatchIn(line) !\u003d None){\r\n        return true\r\n      }\r\n    }\r\n    }catch {\r\n      case ex: Exception \u003d\u003e return false\r\n    }\r\n    return false\r\n  }\r\n  \r\ndef filter(row:Logrow): Boolean \u003d {\r\n      var query \u003d row.query.trim()\r\n      return qaviewFilter(query)\r\n    }\r\n\r\ndef filterQa(x: Option[Logrow]) \u003d x match {\r\n  case Some(s) \u003d\u003e filter(s)\r\n  case None \u003d\u003e false\r\n}\r\n\r\n\r\n\r\ndef filtervr(row:Logrow): Boolean \u003d {\r\n    var flag \u003d true\r\n      for (i \u003c- 0 to row.urls.length - 1) {\r\n        var urlblock \u003d row.urls(i)\r\n        var url \u003d \"\"\r\n        var vrid \u003d  urlblock.get(\"vrid\").get.toString.trim()\r\n\r\n        if (vrid!\u003d\"-1\" \u0026\u0026 vrid!\u003d\"\" \u0026\u0026 !vrid.startsWith(\"500\") \u0026\u0026 !vrid.startsWith(\"300\") \u0026\u0026 !vrid.startsWith(\"800\")) {\r\n          \r\n            return false\r\n        }\r\n        // if (vrid\u003d\u003d\"\") {\r\n        //     return true\r\n        // }\r\n        \r\n        if (i \u003e 3) {\r\n          return true\r\n        }\r\n      }\r\n      return flag\r\n}\r\n\r\ndef filterVR(x: Option[Logrow]) \u003d x match {\r\n  case Some(s) \u003d\u003e filtervr(s)\r\n  case None \u003d\u003e false\r\n}\r\n    \r\n\r\ndef DateFormat(time:String):String\u003d{\r\n    var sdf:SimpleDateFormat \u003d new SimpleDateFormat(\"yyyyMMdd\")\r\n    var date:String \u003d sdf.format(new Date((time.toLong*1000)))\r\n    return date\r\n}\r\n\r\ndef decode(value:String): String \u003d URLDecoder.decode(value, \"gbk\")\r\n\r\ndef getRow(line : String):Option[Logrow]\u003d {\r\n    //    var userid, uuid, page, time, search_type \u003d \"\"\r\n    val regex\u003d\"\"\"^\\d+$\"\"\".r\r\n    var userid, uuid, page,time,search_type,unknown \u003d \"\"\r\n    var tmp \u003d line.trim().split(\u0027\\t\u0027)\r\n    if (tmp.length \u003c 2) {\r\n      return None\r\n    }\r\n    var tmp0 \u003d tmp(0).trim().split(\u0027#\u0027)\r\n    if (tmp0.length !\u003d 5 \u0026\u0026 tmp0.length !\u003d 6) {\r\n      return None\r\n    }\r\n    if(tmp0.length \u003d\u003d 5){\r\n      userid \u003d tmp0(0)\r\n      uuid \u003d tmp0(1)\r\n      page \u003d tmp0(2)\r\n      time \u003d tmp0(3)\r\n      search_type \u003d tmp0(4)\r\n    }else if(tmp0.length \u003d\u003d 6){\r\n      userid \u003d tmp0(0)\r\n      uuid \u003d tmp0(1)\r\n      page \u003d tmp0(2)\r\n      time \u003d tmp0(3)\r\n      search_type \u003d tmp0(4)\r\n      unknown \u003d tmp0(5)\r\n    }\r\n    var query \u003d decode(tmp(1))\r\n    var urls:List[Map[String,String]] \u003d List()\r\n    var urlblock:Map[String,String]\u003d Map()\r\n    var cnt \u003d 0\r\n    for(i \u003c- 2 to tmp.length-1){\r\n      //      println(tmp(i))\r\n      //      println(tmp(i).trim().split(\"#\",-1).length)\r\n      if (((tmp(i).trim().split(\"#\",-1).length) \u003e\u003d 4) \u0026\u0026 regex.findFirstMatchIn((tmp(i).trim().split(\"#\",-1)(1))) !\u003d None){\r\n        var tmp_i \u003d tmp(i).trim().split(\"#\",-1)\r\n        if(tmp_i.length !\u003d 4){\r\n          tmp_i \u003d Array(\"\",\"\",\"\",\"\")\r\n        }\r\n        var Array(vrid, ph_3_1, ph_3_2, baseurl) \u003d tmp_i\r\n        baseurl \u003d decode(baseurl)\r\n        urlblock +\u003d (\"vrid\" -\u003e vrid)\r\n        urlblock +\u003d (\"3_1\" -\u003e ph_3_1)\r\n        urlblock +\u003d (\"3_2\" -\u003e ph_3_2)\r\n        urlblock +\u003d (\"baseurl\" -\u003e baseurl)\r\n        urls \u003d urls :+ urlblock\r\n        urlblock \u003d Map()\r\n        cnt \u003d 0\r\n      }else{\r\n        if(cnt\u003d\u003d0){\r\n          urlblock +\u003d (\"wapurl\" -\u003e tmp(i))\r\n          cnt \u003d cnt+1\r\n        }else if(cnt\u003d\u003d1){\r\n          urlblock +\u003d (\"clk\" -\u003e tmp(i))\r\n          cnt \u003d cnt+1\r\n        }else if(cnt\u003d\u003d2){\r\n          urlblock +\u003d (\"2\" -\u003e tmp(i))\r\n          cnt \u003d cnt+1\r\n        }\r\n      }\r\n    }\r\n    return Some(new Logrow(userid, uuid, page, time, search_type, query, urls))\r\n  }\r\n\r\ndef trans_dataframe(row:Logrow)\u003d{\r\n  var query \u003d row.query\r\n  var clk_o \u003d 0\r\n  var clk_ugc \u003d 0\r\n  for (i \u003c- 0 to row.urls.length - 1) {\r\n    var urlblock \u003d row.urls(i)\r\n    var vrid \u003d  urlblock.get(\"vrid\").get.toString.trim()\r\n    if(vrid \u003d\u003d \"50026601\" || vrid \u003d\u003d \"30000201\" || vrid \u003d\u003d \"50024501\" || vrid \u003d\u003d \"50026501\" || vrid \u003d\u003d \"30000202\" || vrid \u003d\u003d \"30010125\" ){\r\n        clk_ugc +\u003d urlblock.get(\"clk\").getOrElse(\"0\").toInt\r\n    }else{\r\n        clk_o +\u003d urlblock.get(\"clk\").getOrElse(\"0\").toInt\r\n    }\r\n  }\r\n  if(clk_o\u003e0){\r\n      clk_o\u003d1\r\n  }\r\n  (query,clk_ugc,clk_o)\r\n}\r\n\r\ndef filterNone(x: Option[Logrow]) \u003d x match {\r\n      case Some(s) \u003d\u003e true\r\n      case None \u003d\u003e false\r\n    }\r\n\r\n   ",
      "user": "anonymous",
      "dateUpdated": "Jul 18, 2018 11:29:27 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": "org.apache.zeppelin.interpreter.InterpreterException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: 拒绝连接 (Connection refused)",
      "apps": [],
      "jobName": "paragraph_1531815308872_1856777989",
      "id": "20180711-192315_1651061954",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 18, 2018 11:29:27 AM",
      "dateFinished": "Jul 18, 2018 11:29:27 AM",
      "status": "ERROR",
      "errorMessage": "java.net.ConnectException: 拒绝连接 (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:589)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:182)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:92)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:352)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:406)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:329)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "case class Row_qaID(ask:String,ID:String,ans:String)",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:06:07 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined class Row_qaID\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308873_1856393241",
      "id": "20180711-153014_88717447",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 9:06:38 PM",
      "dateFinished": "Jul 17, 2018 9:06:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.hadoopConfiguration.set(\"textinputformat.record.delimiter\",\"\\n\")",
      "dateUpdated": "Jul 17, 2018 4:15:08 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1531815308874_1857547487",
      "id": "20180712-172032_462290675",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// val inputPath \u003d \"/user/webrank/clicklog/ms/201804/*,/user/webrank/clicklog/ms/201805/*,/user/webrank/clicklog/ms/201806/*,/user/webrank/clicklog/ms/201807/*\"\nval inputPath \u003d \"/user/webrank/clicklog/ms/201807/20180711/*\"\nprintln(inputPath)\nvar lograw \u003d sc.textFile(inputPath)",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:06:17 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "inputPath: String \u003d /user/webrank/clicklog/ms/201807/20180711/*\n/user/webrank/clicklog/ms/201807/20180711/*\nlograw: org.apache.spark.rdd.RDD[String] \u003d /user/webrank/clicklog/ms/201807/20180711/* MapPartitionsRDD[1] at textFile at \u003cconsole\u003e:33\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308875_1857162738",
      "id": "20180712-211538_1919452188",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 9:06:43 PM",
      "dateFinished": "Jul 17, 2018 9:06:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var filterQA_RDD \u003d lograw.map(x\u003d\u003egetRow(x)).filter(filterVR).filter(filterQa)",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:06:26 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "filterQA_RDD: org.apache.spark.rdd.RDD[Option[Logrow]] \u003d MapPartitionsRDD[4] at filter at \u003cconsole\u003e:51\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308876_1855238994",
      "id": "20180717-140117_338095396",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 9:06:43 PM",
      "dateFinished": "Jul 17, 2018 9:06:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "filterQA_RDD.take(1)",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 4:17:22 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 124.0,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res20: Array[Option[Logrow]] \u003d Array(Some(Logrow(AAGEsg3IIAAAAAqUG2pbxA4A6gU,000153db-e58a-45e9-95e3-567d26d519eb,1,1531321931,5,保尔柯察金让大家都以为他已经去世了的受伤是什么,List(Map(clk -\u003e 1, baseurl -\u003e http://wenku.baidu.com/view/d34e385f302b3169a45177232f60ddccda38e6c9.html, vrid -\u003e 30000802, 3_1 -\u003e 2, wapurl -\u003e http://wk.baidu.com/view/d34e385f302b3169a45177232f60ddccda38e6c9, 2 -\u003e 1531322591, 3_2 -\u003e 0), Map(clk -\u003e 0, baseurl -\u003e http://ishare.iask.sina.com.cn/f/310lAK5ViLT.html, vrid -\u003e 30000909, 3_1 -\u003e 2, wapurl -\u003e http://m.ishare.iask.sina.com.cn/f/310lAK5ViLT.html, 2 -\u003e -1, 3_2 -\u003e 0), Map(clk -\u003e 0, baseurl -\u003e http://wenwen.sogou.com/z/q806616530.htm, vrid -\u003e 30000201, 3_1 -\u003e 2, wapurl -\u003e http://wenwen.sogou.com/z/q806616530.htm, 2 -\u003e -1, 3_2 -\u003e 0), Map(clk -\u003e 0, baseurl -\u003e , vrid -\u003e 50023801, 3_1 -\u003e ..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308877_1854854245",
      "id": "20180717-143243_2029174316",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 4:17:22 PM",
      "dateFinished": "Jul 17, 2018 4:17:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "filterQA_RDD.cache()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:07:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res31: org.apache.spark.rdd.RDD[Option[Logrow]] \u003d MapPartitionsRDD[4] at filter at \u003cconsole\u003e:51\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531832857821_-937726522",
      "id": "20180717-210737_649446645",
      "dateCreated": "Jul 17, 2018 9:07:37 PM",
      "dateStarted": "Jul 17, 2018 9:07:44 PM",
      "dateFinished": "Jul 17, 2018 9:07:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var result \u003d filterQA_RDD.map(x\u003d\u003ex.get).map(trans_dataframe).map(r \u003d\u003e DFrow(r._1,r._2.toInt,r._3.toInt)).toDF()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:07:49 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "result: org.apache.spark.sql.DataFrame \u003d [query: string, clk_ugc: int ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308879_1855623743",
      "id": "20180717-140331_2024293077",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 9:07:49 PM",
      "dateFinished": "Jul 17, 2018 9:07:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "filterVR_RDD.take(10)",
      "dateUpdated": "Jul 17, 2018 4:15:08 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res33: Array[Option[Logrow]] \u003d Array(Some(Logrow(c7f18f7c64ae803c640a64de10d598af,0000bbb3-bcf2-4406-a5eb-330f55e9a896,1,1530438071,5,平安京万年竹1技能暴击,List(Map(clk -\u003e 1, baseurl -\u003e http://news.4399.com/jzpaj/xinde/m/824612.html, vrid -\u003e 30000909, 3_1 -\u003e 2, wapurl -\u003e http://news.4399.com/mobile/jzpaj/xinde/m/824612.html, 2 -\u003e 1530438073, 3_2 -\u003e 0), Map(clk -\u003e 0, baseurl -\u003e https://www.520apk.com/shoujiyouxi/shouyouwenda/151869.html, vrid -\u003e 30000909, 3_1 -\u003e 2, wapurl -\u003e https://m.520apk.com/zixun/151869.html, 2 -\u003e -1, 3_2 -\u003e 0), Map(clk -\u003e 0, baseurl -\u003e http://www.gamedog.cn/games/a/2408814.html, vrid -\u003e 30000909, 3_1 -\u003e 2, wapurl -\u003e http://m.gamedog.cn/games/a/2408814.html, 2 -\u003e -1, 3_2 -\u003e 0), Map(clk -\u003e 0, baseurl -\u003e , vrid -\u003e 50023801, 3_1 -\u003e 1, wapurl -\u003e http://www.sogou.com/, 2 -\u003e -1, 3_..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308880_1866011963",
      "id": "20180717-142517_1737618919",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "result.show()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 4:19:00 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+-------+-----+\n|               query|clk_ugc|clk_o|\n+--------------------+-------+-----+\n|保尔柯察金让大家都以为他已经去世了...|      0|    1|\n| 同一个手机号可以绑定多张相同银行的卡吗|      0|    0|\n|       三叉神经痛可以波及到双膝吗|      0|    1|\n|    有什么办法可以给华为手机应用锁吗|      0|    1|\n|             火龙果刺有毒吗|      0|    1|\n|         树莓用白糖一起吃可以吗|      0|    1|\n|           乐敦清孕妇可以用吗|      0|    0|\n|            孕妇可以吃橘子吗|      0|    1|\n|              肾宝片有用吗|      0|    1|\n|             有没有黄色网站|      0|    1|\n|             跑步机能减肥吗|      1|    0|\n|          南通苏建花园城怎么样|      0|    1|\n|           怀孕血糖高能喝粥吗|      0|    0|\n|   开发商有权私自把顶层楼梯封死自用吗|      0|    0|\n| 第一次做生产文员每天要问老板要做什么吗|      0|    0|\n|        凤城和凤凰城是一个地方吗|      0|    0|\n|          wis面膜有荧光剂吗|      0|    1|\n|            广西骨伤医院好吗|      0|    0|\n|        肾结石积水肾变形会怎么样|      0|    1|\n| 抗丙肝病毒抗体是0.13（一）是正常吗|      0|    0|\n+--------------------+-------+-----+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308884_1864472967",
      "id": "20180712-113519_1873369492",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 4:19:00 PM",
      "dateFinished": "Jul 17, 2018 4:19:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var result_ugc \u003d result.where(\"clk_ugc \u003e 0\")",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:08:04 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "result_ugc: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [query: string, clk_ugc: int ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815646217_801884638",
      "id": "20180717-162046_1625050168",
      "dateCreated": "Jul 17, 2018 4:20:46 PM",
      "dateStarted": "Jul 17, 2018 9:08:04 PM",
      "dateFinished": "Jul 17, 2018 9:08:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "result_ugc.cache()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:08:34 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res32: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [query: string, clk_ugc: int ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531816937680_1240924695",
      "id": "20180717-164217_1664325313",
      "dateCreated": "Jul 17, 2018 4:42:17 PM",
      "dateStarted": "Jul 17, 2018 9:08:34 PM",
      "dateFinished": "Jul 17, 2018 9:08:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "result_ugc.show()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 4:21:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------+-------+-----+\n|         query|clk_ugc|clk_o|\n+--------------+-------+-----+\n|       跑步机能减肥吗|      1|    0|\n|  一个手机上能用两个微信吗|      1|    0|\n|一部手机能同时登录两个微信吗|      1|    1|\n|    火车上有充电的地方吗|      1|    1|\n|     有一辈子的婚外恋吗|      1|    0|\n|      坟头上长树好不好|      1|    1|\n|     科龙空调质量怎么样|      1|    1|\n|     韩国是不是发达国家|      1|    1|\n|       仓鼠能吃西瓜吗|      1|    1|\n|  能不能使用公积金贷款买车|      1|    0|\n|      世界上真的有龙吗|      1|    1|\n|    黑豆绿豆可以一起煮吗|      1|    1|\n|      可以代办银行卡吗|      1|    0|\n|        松香是晶体吗|      1|    0|\n|       k火车能充电吗|      1|    1|\n|      虎皮鹦鹉会说话吗|      1|    1|\n|   psp可以玩口袋妖怪吗|      1|    0|\n|       办银行卡要钱吗|      1|    0|\n|     浙江科技学院好不好|      1|    0|\n|        人肉可以吃吗|      1|    0|\n+--------------+-------+-----+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815699709_1984076801",
      "id": "20180717-162139_1234078963",
      "dateCreated": "Jul 17, 2018 4:21:39 PM",
      "dateStarted": "Jul 17, 2018 4:21:44 PM",
      "dateFinished": "Jul 17, 2018 4:22:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "result_ugc.count()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:08:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res33: Long \u003d 1427261\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815716989_1108018880",
      "id": "20180717-162156_852066725",
      "dateCreated": "Jul 17, 2018 4:21:56 PM",
      "dateStarted": "Jul 17, 2018 9:08:40 PM",
      "dateFinished": "Jul 17, 2018 9:12:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "result_ugc.registerTempTable(\"t_log\")",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:08:44 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308885_1864088219",
      "id": "20180712-113634_95412830",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 9:08:45 PM",
      "dateFinished": "Jul 17, 2018 9:12:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "Sep 4, 2018 4:12:09 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1531816419351_849127198",
      "id": "20180717-163339_1052776971",
      "dateCreated": "Jul 17, 2018 4:33:39 PM",
      "dateStarted": "Jul 17, 2018 9:12:40 PM",
      "dateFinished": "Jul 17, 2018 9:12:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.hadoopConfiguration.set(\"textinputformat.record.delimiter\",\"\\n@\\n\")\r\ntauaPath \u003d \"hdfs://master004.diablo.hadoop.nm.ted:8020/user/webkm/xuen/lizhi_temp/qapairs\"\r\nval raw \u003d sc.textFile(tauaPath)",
      "user": "anonymous",
      "dateUpdated": "Sep 4, 2018 4:12:17 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "tauaPath: String \u003d hdfs://master004.diablo.hadoop.nm.ted:8020/user/webkm/xuen/lizhi_temp/qapairs\nraw: org.apache.spark.rdd.RDD[String] \u003d hdfs://master004.diablo.hadoop.nm.ted:8020/user/webkm/xuen/lizhi_temp/qapairs MapPartitionsRDD[21] at textFile at \u003cconsole\u003e:33\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531816442396_1003811580",
      "id": "20180717-163402_234485827",
      "dateCreated": "Jul 17, 2018 4:34:02 PM",
      "dateStarted": "Jul 17, 2018 9:12:40 PM",
      "dateFinished": "Jul 17, 2018 9:12:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "raw.take(1)",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 4:55:54 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res48: Array[String] \u003d\nArray(@ACCEPT:false\n@ANS.CONTENT:现在很少人用嵌体补牙了，主要是技术要求高，做的完全吻合的嵌体太难，一般都是用玻璃离子或树脂补牙，然后做个烤瓷冠或全瓷冠。\u003cbr\u003e补牙的费用主要由所用补牙材料决定，各地价格有一定差别。以我们山东临沂来说，临沂口腔医院国产玻璃离子补牙大概30元/颗，进口玻璃离子补牙120-150元/颗，树脂补牙，国产一般80元左右一颗，进口一般200元左右一颗。如果龋齿比较严重，可能还涉及到根管治疗和做保护冠，费用比补牙要高。\u003cbr\u003e烤瓷牙的种类非常多，还有更好一点的二氧化锆全瓷牙。拿我们当地的临沂口腔医院来说：低端的镍铬烤瓷牙一般两三百元/颗，中端的钴铬烤瓷牙一般六七百/颗，纯钛烤瓷牙一千五左右/颗，金合金烤瓷牙两三千/颗，比较好的是氧化锆全瓷牙国产的一般1500-3000元/颗，进口的一般三千以上。比较知名的进口品牌有德国维兰德、德国KAVA、美国3M LAVA。\n@ANS.DATE:2015-08-13 08:10:22\n@ANS.ID:830b7a1af6133e1c47837b2412a9ce38\n@ASK.DATE:2015-08-13 08:10:22\n@ASK.HTYPE:UN\n@ASK.HTYPE_LEVEL:9.7893\n@ASK.ID:000005fb9ee5eaf4e00edf69d4ed2a8e\n@ASK.TITLE:补牙嵌体冠的价位有多少，最便宜多少钱\n@AWR.NAME:热心问友\n@GROUP.ID:000005fb9ee5eaf4e00edf69d4ed2a8e\n@ID:a7357cdd23d7405c4dad0df43b393495\n@OP_NUM:0\n@QER.NAME:匿名\n@SOURCE:sogou_wenwen\n@SP_NUM:0\n@U..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531817535489_1509501330",
      "id": "20180717-165215_876747786",
      "dateCreated": "Jul 17, 2018 4:52:15 PM",
      "dateStarted": "Jul 17, 2018 4:55:54 PM",
      "dateFinished": "Jul 17, 2018 4:55:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " var tall \u003d raw.map(x \u003d\u003elineToQA(x)).map(r \u003d\u003e Row_qaID(r._1,r._2,r._3)).toDF() ",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:08:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "tall: org.apache.spark.sql.DataFrame \u003d [ask: string, ID: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531816845867_1630736628",
      "id": "20180717-164045_796221646",
      "dateCreated": "Jul 17, 2018 4:40:45 PM",
      "dateStarted": "Jul 17, 2018 9:12:41 PM",
      "dateFinished": "Jul 17, 2018 9:12:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "tall \u003d tall.dropDuplicates(Seq(\"ask\",\"ID\",\"ans\"))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:00 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "tall: org.apache.spark.sql.DataFrame \u003d [ask: string, ID: string ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531817180373_1211236089",
      "id": "20180717-164620_666144790",
      "dateCreated": "Jul 17, 2018 4:46:20 PM",
      "dateStarted": "Jul 17, 2018 9:12:41 PM",
      "dateFinished": "Jul 17, 2018 9:12:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "tall.registerTempTable(\"t_all\")",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:07 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531817240633_-419855705",
      "id": "20180717-164720_83232495",
      "dateCreated": "Jul 17, 2018 4:47:20 PM",
      "dateStarted": "Jul 17, 2018 9:12:42 PM",
      "dateFinished": "Jul 17, 2018 9:12:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sqlContext.udf.register(\"queryhashc\",queryHash)",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:10 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res37: org.apache.spark.sql.expressions.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,StringType,Some(List(StringType)))\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531817118683_1628026417",
      "id": "20180717-164518_725376389",
      "dateCreated": "Jul 17, 2018 4:45:18 PM",
      "dateStarted": "Jul 17, 2018 9:12:42 PM",
      "dateFinished": "Jul 17, 2018 9:12:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var sql_result4 \u003d sqlContext.sql(\"SELECT ask,ans FROM t_log join t_all on t_all.ID\u003dqueryhashc(t_log.query)\")",
      "user": "anonymous",
      "dateUpdated": "Jul 18, 2018 11:29:02 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": "org.apache.thrift.transport.TTransportException",
      "apps": [],
      "jobName": "paragraph_1531817134134_-317248698",
      "id": "20180717-164534_344852772",
      "dateCreated": "Jul 17, 2018 4:45:34 PM",
      "dateStarted": "Jul 18, 2018 11:29:02 AM",
      "dateFinished": "Jul 18, 2018 11:29:12 AM",
      "status": "ERROR",
      "errorMessage": "org.apache.thrift.transport.TTransportException\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_interpret(RemoteInterpreterService.java:266)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.interpret(RemoteInterpreterService.java:250)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:373)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:406)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:329)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result4.show()",
      "user": "anonymous",
      "dateUpdated": "Jul 18, 2018 11:22:57 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": "org.apache.thrift.transport.TTransportException",
      "apps": [],
      "jobName": "paragraph_1531817317062_792796207",
      "id": "20180717-164837_1192587051",
      "dateCreated": "Jul 17, 2018 4:48:37 PM",
      "dateStarted": "Jul 18, 2018 11:22:57 AM",
      "dateFinished": "Jul 18, 2018 11:23:09 AM",
      "status": "ERROR",
      "errorMessage": "org.apache.thrift.transport.TTransportException\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_interpret(RemoteInterpreterService.java:266)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.interpret(RemoteInterpreterService.java:250)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:373)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:406)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:329)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result4 \u003d sql_result4.dropDuplicates(Seq(\"ask\",\"ans\"))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:19 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "sql_result4: org.apache.spark.sql.DataFrame \u003d [ask: string, ans: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531819997405_225904546",
      "id": "20180717-173317_1917781559",
      "dateCreated": "Jul 17, 2018 5:33:17 PM",
      "dateStarted": "Jul 17, 2018 9:12:42 PM",
      "dateFinished": "Jul 17, 2018 9:12:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result4.cache()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:23 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange(coordinator id: 441393960) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n+- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n   +- *Project [ask#70, ans#72]\n      +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n         :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n         :  +- InMemoryTableScan [query#16]\n         :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         :              +- *Filter (clk_ugc#17 \u003e 0)\n         :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n         :                    +- Scan ExternalRDDScan[obj#15]\n         +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n            +- Exchange(coordinator id: 2067482528) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n               +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                  +- *Filter isnotnull(ID#71)\n                     +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                        +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation.buildBuffers(InMemoryRelation.scala:91)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation.\u003cinit\u003e(InMemoryRelation.scala:86)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:42)\n  at org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1.apply(CacheManager.scala:100)\n  at org.apache.spark.sql.execution.CacheManager.writeLock(CacheManager.scala:68)\n  at org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:92)\n  at org.apache.spark.sql.Dataset.persist(Dataset.scala:2513)\n  at org.apache.spark.sql.Dataset.cache(Dataset.scala:2523)\n  ... 46 elided\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [3000 seconds]\n  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultCode(HashAggregateExec.scala:477)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:612)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:600)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.doEstimationIfNecessary(ExchangeCoordinator.scala:211)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.postShuffleRDD(ExchangeCoordinator.scala:259)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:120)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 70 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531820060209_1766747270",
      "id": "20180717-173420_1678144260",
      "dateCreated": "Jul 17, 2018 5:34:20 PM",
      "dateStarted": "Jul 17, 2018 9:12:43 PM",
      "dateFinished": "Jul 17, 2018 10:02:44 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result4.count()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 10:02:45 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *HashAggregate(keys\u003d[], functions\u003d[partial_count(1)], output\u003d[count#151L])\n   +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[])\n      +- Exchange(coordinator id: 2009441391) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n         +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n            +- *Project [ask#70, ans#72]\n               +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n                  :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n                  :  +- InMemoryTableScan [query#16]\n                  :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n                  :              +- *Filter (clk_ugc#17 \u003e 0)\n                  :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n                  :                    +- Scan ExternalRDDScan[obj#15]\n                  +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                     +- Exchange(coordinator id: 2029683524) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                        +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                           +- *Filter isnotnull(ID#71)\n                              +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                                 +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n  ... 47 elided\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange(coordinator id: 2009441391) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n+- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n   +- *Project [ask#70, ans#72]\n      +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n         :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n         :  +- InMemoryTableScan [query#16]\n         :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         :              +- *Filter (clk_ugc#17 \u003e 0)\n         :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n         :                    +- Scan ExternalRDDScan[obj#15]\n         +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n            +- Exchange(coordinator id: 2029683524) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n               +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                  +- *Filter isnotnull(ID#71)\n                     +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                        +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 71 more\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [3000 seconds]\n  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultCode(HashAggregateExec.scala:477)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:612)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:600)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.doEstimationIfNecessary(ExchangeCoordinator.scala:211)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.postShuffleRDD(ExchangeCoordinator.scala:259)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:120)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 92 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531832975125_773388110",
      "id": "20180717-210935_1409494467",
      "dateCreated": "Jul 17, 2018 9:09:35 PM",
      "dateStarted": "Jul 17, 2018 10:02:45 PM",
      "dateFinished": "Jul 17, 2018 11:42:46 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result4.count()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 5:39:46 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res56: Long \u003d 1973568\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531820031762_-7976812",
      "id": "20180717-173351_1500952023",
      "dateCreated": "Jul 17, 2018 5:33:51 PM",
      "dateStarted": "Jul 17, 2018 5:39:46 PM",
      "dateFinished": "Jul 17, 2018 5:50:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result4.show()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:27 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange(coordinator id: 1942605957) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n+- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n   +- *Project [ask#70, ans#72]\n      +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n         :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n         :  +- InMemoryTableScan [query#16]\n         :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         :              +- *Filter (clk_ugc#17 \u003e 0)\n         :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n         :                    +- Scan ExternalRDDScan[obj#15]\n         +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n            +- Exchange(coordinator id: 718048806) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n               +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                  +- *Filter isnotnull(ID#71)\n                     +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                        +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:311)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:644)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:603)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:612)\n  ... 47 elided\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [3000 seconds]\n  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultCode(HashAggregateExec.scala:477)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:612)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:600)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.doEstimationIfNecessary(ExchangeCoordinator.scala:211)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.postShuffleRDD(ExchangeCoordinator.scala:259)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:120)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 78 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531821274676_-1564137281",
      "id": "20180717-175434_2070588221",
      "dateCreated": "Jul 17, 2018 5:54:34 PM",
      "dateStarted": "Jul 17, 2018 9:12:44 PM",
      "dateFinished": "Jul 17, 2018 10:52:45 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result4.select(\"ask\").distinct.count()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:42 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *HashAggregate(keys\u003d[], functions\u003d[partial_count(1)], output\u003d[count#177L])\n   +- *HashAggregate(keys\u003d[ask#70], functions\u003d[], output\u003d[])\n      +- Exchange(coordinator id: 1720260875) hashpartitioning(ask#70, 200), coordinator[target post-shuffle partition size: 67108864]\n         +- *HashAggregate(keys\u003d[ask#70], functions\u003d[], output\u003d[ask#70])\n            +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70])\n               +- Exchange(coordinator id: 767205482) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                  +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n                     +- *Project [ask#70, ans#72]\n                        +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n                           :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n                           :  +- InMemoryTableScan [query#16]\n                           :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n                           :              +- *Filter (clk_ugc#17 \u003e 0)\n                           :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n                           :                    +- Scan ExternalRDDScan[obj#15]\n                           +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                              +- Exchange(coordinator id: 538021639) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                                 +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                                    +- *Filter isnotnull(ID#71)\n                                       +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                                          +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n  ... 47 elided\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange(coordinator id: 1720260875) hashpartitioning(ask#70, 200), coordinator[target post-shuffle partition size: 67108864]\n+- *HashAggregate(keys\u003d[ask#70], functions\u003d[], output\u003d[ask#70])\n   +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70])\n      +- Exchange(coordinator id: 767205482) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n         +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n            +- *Project [ask#70, ans#72]\n               +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n                  :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n                  :  +- InMemoryTableScan [query#16]\n                  :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n                  :              +- *Filter (clk_ugc#17 \u003e 0)\n                  :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n                  :                    +- Scan ExternalRDDScan[obj#15]\n                  +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                     +- Exchange(coordinator id: 538021639) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                        +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                           +- *Filter isnotnull(ID#71)\n                              +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                                 +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 71 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange(coordinator id: 767205482) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n+- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n   +- *Project [ask#70, ans#72]\n      +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n         :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n         :  +- InMemoryTableScan [query#16]\n         :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         :              +- *Filter (clk_ugc#17 \u003e 0)\n         :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n         :                    +- Scan ExternalRDDScan[obj#15]\n         +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n            +- Exchange(coordinator id: 538021639) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n               +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                  +- *Filter isnotnull(ID#71)\n                     +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                        +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.doEstimationIfNecessary(ExchangeCoordinator.scala:211)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.postShuffleRDD(ExchangeCoordinator.scala:259)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:120)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 92 more\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [3000 seconds]\n  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultCode(HashAggregateExec.scala:477)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:612)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:600)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.doEstimationIfNecessary(ExchangeCoordinator.scala:211)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.postShuffleRDD(ExchangeCoordinator.scala:259)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:120)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 115 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308900_1846005020",
      "id": "20180712-190235_1395179735",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 10:52:46 PM",
      "dateFinished": "Jul 18, 2018 12:32:47 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var sql_result3 \u003d sql_result4.dropDuplicates(Seq(\"ask\"))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:46 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "sql_result3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [ask: string, ans: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531821443949_1746286583",
      "id": "20180717-175723_167439323",
      "dateCreated": "Jul 17, 2018 5:57:23 PM",
      "dateStarted": "Jul 18, 2018 12:32:47 AM",
      "dateFinished": "Jul 18, 2018 12:32:47 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result3.createOrReplaceTempView(\"sql_result3\")\nvar sql_result \u003dsqlContext.sql(\"SELECT CONCAT(\u0027@ASK.TITLE:\u0027,ask,\u0027\\t@ANS.CONTENT:\u0027,ans) as line FROM sql_result3\")",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:09:53 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "sql_result: org.apache.spark.sql.DataFrame \u003d [line: string]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308905_1844081276",
      "id": "20180712-200356_66876918",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 18, 2018 12:32:47 AM",
      "dateFinished": "Jul 18, 2018 12:32:49 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result.select(\"line\").rdd.saveAsTextFile(\"/user/webrank/liuqin/statistic/query_ans_eval_distinct.csv\")",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 9:10:20 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nSortAggregate(key\u003d[ask#70], functions\u003d[first(ans#72, false)], output\u003d[ask#70, ans#72])\n+- *Sort [ask#70 ASC NULLS FIRST], false, 0\n   +- Exchange(coordinator id: 1965096389) hashpartitioning(ask#70, 200), coordinator[target post-shuffle partition size: 67108864]\n      +- SortAggregate(key\u003d[ask#70], functions\u003d[partial_first(ans#72, false)], output\u003d[ask#70, first#204, valueSet#205])\n         +- *Sort [ask#70 ASC NULLS FIRST], false, 0\n            +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n               +- Exchange(coordinator id: 1135718311) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                  +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n                     +- *Project [ask#70, ans#72]\n                        +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n                           :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n                           :  +- InMemoryTableScan [query#16]\n                           :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n                           :              +- *Filter (clk_ugc#17 \u003e 0)\n                           :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n                           :                    +- Scan ExternalRDDScan[obj#15]\n                           +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                              +- Exchange(coordinator id: 1972579305) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                                 +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                                    +- *Filter isnotnull(ID#71)\n                                       +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                                          +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:75)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:95)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n  at org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2581)\n  at org.apache.spark.sql.Dataset.rdd(Dataset.scala:2578)\n  ... 47 elided\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange(coordinator id: 1965096389) hashpartitioning(ask#70, 200), coordinator[target post-shuffle partition size: 67108864]\n+- SortAggregate(key\u003d[ask#70], functions\u003d[partial_first(ans#72, false)], output\u003d[ask#70, first#204, valueSet#205])\n   +- *Sort [ask#70 ASC NULLS FIRST], false, 0\n      +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n         +- Exchange(coordinator id: 1135718311) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n            +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n               +- *Project [ask#70, ans#72]\n                  +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n                     :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n                     :  +- InMemoryTableScan [query#16]\n                     :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n                     :              +- *Filter (clk_ugc#17 \u003e 0)\n                     :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n                     :                    +- Scan ExternalRDDScan[obj#15]\n                     +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                        +- Exchange(coordinator id: 1972579305) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                           +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                              +- *Filter isnotnull(ID#71)\n                                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                                    +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:77)\n  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:75)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 74 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nSortAggregate(key\u003d[ask#70], functions\u003d[partial_first(ans#72, false)], output\u003d[ask#70, first#204, valueSet#205])\n+- *Sort [ask#70 ASC NULLS FIRST], false, 0\n   +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n      +- Exchange(coordinator id: 1135718311) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n         +- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n            +- *Project [ask#70, ans#72]\n               +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n                  :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n                  :  +- InMemoryTableScan [query#16]\n                  :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n                  :              +- *Filter (clk_ugc#17 \u003e 0)\n                  :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n                  :                    +- Scan ExternalRDDScan[obj#15]\n                  +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                     +- Exchange(coordinator id: 1972579305) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n                        +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                           +- *Filter isnotnull(ID#71)\n                              +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                                 +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:75)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.doEstimationIfNecessary(ExchangeCoordinator.scala:211)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.postShuffleRDD(ExchangeCoordinator.scala:259)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:120)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 93 more\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange(coordinator id: 1135718311) hashpartitioning(ask#70, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n+- *HashAggregate(keys\u003d[ask#70, ans#72], functions\u003d[], output\u003d[ask#70, ans#72])\n   +- *Project [ask#70, ans#72]\n      +- *BroadcastHashJoin [UDF:queryhashc(query#16)], [ID#71], Inner, BuildLeft\n         :- BroadcastExchange HashedRelationBroadcastMode(List(UDF:queryhashc(input[0, string, true])))\n         :  +- InMemoryTableScan [query#16]\n         :        +- InMemoryRelation [query#16, clk_ugc#17, clk_o#18], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n         :              +- *Filter (clk_ugc#17 \u003e 0)\n         :                 +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, DFrow, true]).query, true) AS query#16, assertnotnull(input[0, DFrow, true]).clk_ugc AS clk_ugc#17, assertnotnull(input[0, DFrow, true]).clk_o AS clk_o#18]\n         :                    +- Scan ExternalRDDScan[obj#15]\n         +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n            +- Exchange(coordinator id: 1972579305) hashpartitioning(ask#70, ID#71, ans#72, 200), coordinator[target post-shuffle partition size: 67108864]\n               +- *HashAggregate(keys\u003d[ask#70, ID#71, ans#72], functions\u003d[], output\u003d[ask#70, ID#71, ans#72])\n                  +- *Filter isnotnull(ID#71)\n                     +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ask, true) AS ask#70, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ID, true) AS ID#71, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Row_qaID, true]).ans, true) AS ans#72]\n                        +- Scan ExternalRDDScan[obj#69]\n\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:77)\n  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:75)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 106 more\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [3000 seconds]\n  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultCode(HashAggregateExec.scala:477)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:612)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:600)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.doEstimationIfNecessary(ExchangeCoordinator.scala:211)\n  at org.apache.spark.sql.execution.exchange.ExchangeCoordinator.postShuffleRDD(ExchangeCoordinator.scala:259)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:120)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  ... 126 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531815308906_1845235522",
      "id": "20180712-200705_1166942747",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 18, 2018 12:32:48 AM",
      "dateFinished": "Jul 18, 2018 1:22:51 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sql_result.count()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 6:15:39 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res74: Long \u003d 22166\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1531821714127_200930329",
      "id": "20180717-180154_1858355949",
      "dateCreated": "Jul 17, 2018 6:01:54 PM",
      "dateStarted": "Jul 17, 2018 6:15:39 PM",
      "dateFinished": "Jul 17, 2018 6:15:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.stop()",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2018 6:53:56 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1531815308908_1842927029",
      "id": "20180516-191133_1792986149",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "dateStarted": "Jul 17, 2018 6:53:56 PM",
      "dateFinished": "Jul 17, 2018 6:53:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jul 17, 2018 4:15:08 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1531815308909_1842542280",
      "id": "20180712-112412_359117437",
      "dateCreated": "Jul 17, 2018 4:15:08 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "medical/medical_eval_sample",
  "id": "2DNAK6M38",
  "angularObjects": {
    "2D9M8ATZ9:shared_process": [],
    "2D859SF5B:shared_process": [],
    "2D99W32FC:shared_process": [],
    "2DA8NG9YB:shared_process": [],
    "2DBCA9BMV:shared_process": [],
    "2DA29EQ39:shared_process": [],
    "2D86PKHDE:shared_process": [],
    "2D8ZMX5FY:shared_process": [],
    "2D8ZFKME2:shared_process": [],
    "2DBAZD2WP:shared_process": [],
    "2D8SP4FH8:shared_process": [],
    "2DAESRJYD:shared_process": [],
    "2DA7377EZ:shared_process": [],
    "2D8DH9K51:shared_process": [],
    "2D85K8KV7:shared_process": [],
    "2D9NTGN5D::2DNAK6M38": [],
    "2D958F7RN:shared_process": [],
    "2DAVR7XRG:shared_process": [],
    "2DBX9FA55:shared_process": []
  },
  "config": {},
  "info": {}
}